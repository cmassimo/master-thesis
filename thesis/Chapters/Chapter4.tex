% Chapter 4

\chapter{Experiments}
\label{Chapter4}

In order to assess the quality of the proposed methodology we tested it on a
number of datasets and against a number of baselines that will be detailed in
the following sections.

\section{Datasets}
\label{subsec:datasets}

The experiments were conducted on five real-world applications datasets, namely:
AIDS \cite{Weislow19041989}, CAS\footnote{http://cheminformatics.org/datasets/bursi/},
CPDB \cite{journals/jcisd/HelmaCKR04}, GDD \cite{dobson2003}, and NCI1 \cite{journals/kais/WaleWK08}.
All these datasets contains chemical and molecular particles encoded in graph form.
The AIDS Antiviral Screen dataset contains chemical compounds, categorized
into chemical groups; CAS and CPDB are dataset of mutagenic
compounds; NCI1 consists of chemical compounds screened for activity against 
non-small lung cancer cells; GDD is composed of X-ray crystal structures of
proteins represented as graphs.
The graph encoding of this data is such that every node is labelled and none
has a self loop that is there are no nodes with an edge going out and going back
into them.
Some statistics about these datasets are gathered in table~\ref{table:datasets}.

    \begin{table}[ht]
        \centering
        \begin{tabular}{|r|r|r|r|r|}
            \hline
            Dataset & n. of graphs & sample split & avg nodes & avg edges \\ \hline
            AIDS    & 1503         & 28.07        & 58.90     & 61.40     \\ \hline      
            CAS     & \textbf{4337} & 55.36        & 29.90     & 30.90     \\ \hline      
            CPDB    &  684         & 49.85        & 14.10     & 14.60     \\ \hline      
            GDD     & 1503         & 58.65        & 284.31    & \textbf{2862.63}   \\ \hline      
            NCI1    & 1503         & 50.04        & 29.87     & 32.30     \\ \hline      
        \end{tabular}
        \caption{Statistics about the datasets employed in the experiments: number
        of graphs, labels percentage among samples, average number of nodes, average
        number of edges. It is clear from this table that the CAS and NCI1 datasets
        are the bigger ones while the GDD holds the most complex graphs either in
        terms of topography and processing. Furthermore the AIDS dataset turns out
        to be quite unbalanced in terms of labels distribution \cite{rtesselli}.}
        \label{table:datasets}
    \end{table}

%----------------------------------------------------------------------------------------

\section{Experiments description}
\label{sec:description}

% add more motivations behind these experiments.
The $k$ parameter of the cross-validation has been fixed at 10 since this value
already provides good statistical significance while helping containing the bias
skewing during the training phase.
Moreover, the whole routine has been run ten times with ten different split of the
data to mitigate the high variance derived during the testing phase due to the
chosen value of $k$.

The original kernel combinations selected for this study were:
\begin{enumerate}
    \item the $ODDK_{ST}$ and $TCK_{ST}$ graph kernels,
    \item the $ODDK_{ST+}$ and $TCK_{ST+}$ graph kernels,
    \item the $WL$ fast subtree and $WLC$ graph kernels,
\end{enumerate}

for each combination a separated set of experiment was conducted, while maintaining
the same structure, of which a detailed breakdown will be given in Table \ref{table:structure}.

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|l|}
        \hline
        exp. no. & method & kernels \\
        \hline
        1 & easyMKL & $K$ and $K'$ \\
        \hline
        2 & easyMKL & $K'$ \\
        \hline
        3 & easyMKL & $K$ \\
        \hline
    \end{tabular}
    \caption{Structure of the main experiments. Kernels $K$ and $K'$ refer
    to the version without and with contexts for each combination respectively.
    These kernels generated a set of matrices each, according to the technique
    described in Section \ref{subsec:features} which were concatenated as a list
    to be given in input to easyMKL.}
    \label{table:structure}
\end{table}

The hyper-parameter selection process for the kernels has been embedded
into the MKL learning phase so each set of kernel matrices has been
pre-computed for each one of the possible combination of parameters that would
otherwise have been used in a grid-search fashion.
For all the following experiments, the $\Lambda$ parameter of easyMKL has been
validated from the set $\{0.0, 0.1,\dots,1.0\}$ during the cross-validation
routine.

\paragraph{The case of the GDD dataset:}
\label{par:gdd}
confirming the experience reported in \cite{rtesselli}, while working with the GDD
dataset and the $ODD$ kernels we had to limit the $h$ hyper-parameter to the set
$\{1,2,3\}$ because for heights greater than 3 the computational times of the
kernel matrices became prohibitive, due to the high complexity and magnitude
of the data structures contained in this dataset.


\subsection[$ODDK_{ST}$ and $TCK_{ST}$]{$\boldsymbol{ODDK_{ST}}$ and $\boldsymbol{TCK_{ST}}$}
The kernels used for this experiment were generated from the $ODDK$ kernels
in \cite{DBLP:conf/sdm/MartinoNS12, Navarin2015}, namely $ODDK_{ST}$, $TCK_{ST}$.
Since EasyMKL took care of weighing the individual kernels, the $\lambda$
parameters of the ODD kernels has been fixed to 1, while the $h$ parameter
values were the set $\{1,\dots,10\}$.

\subsection[$ODDK_{ST+}$ and $TCK_{ST+}$]{$\boldsymbol{ODDK_{ST+}}$ and $\boldsymbol{TCK_{ST+}}$}
The kernels used for this experiment were generated from the $ODDK$ kernels
in \cite{dasanmartino2015exploiting, rtesselli}, namely $ODDK_{ST+}$, $TCK_{ST+}$.
Again, EasyMKL took care of weighing the individual kernels so the $\lambda$
parameters of the ODD kernels has been fixed to 1, while the $h$ parameter
values were the set $\{1,\dots,10\}$.
During the cross-validation routine, the $\Lambda$ parameter of easyMKL has been
validated from the set $\{0.0, 0.1,\dots,1.0\}$.

\subsection[$WL$ and $WLC$]{$\boldsymbol{WLK}$ and $\boldsymbol{WLCK}$}
The kernels used for this experiment were generated from the $Fast~Subtree$ kernels
in \cite{NIPS2009_3813, rtesselli}, namely the $WL$ kernel and the $WLC$ kernel.
These two kernels only need to validate one hyper-parameter, that is $h$ the iterations
limit whose values were those in the set $\{1,\dots,10\}$.
During the cross-validation routine, the $\Lambda$ parameter of easyMKL has been
validated from the set $\{0.0, 0.1,\dots,1.0\}$.

\subsection{Space resources requirements}
Given the large number of kernel matrices involved in the combination exepriments
we would like to detail the space resources requirements that these experiments
had (Table \ref{table:space}) with the above mentioned setup.
Here we show only the data concerning the $ODD$ kernels since the $WL$ kernels
generated a number of matrices an order of magnitude less than the former.

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|}
        \hline
        exp. no. & method [kernel(s)] & AIDS & CAS & CPDB & GDD & NCI1 \\
        \hline
        1 & easyMKL [$K$ and $K'$] & 5 GB & 34 GB & 3 GB & 3 GB & 29 GB \\
        \hline
        2 & easyMKL [$K'$] & 3 GB & 19 GB & 2 GB & 1 GB & 17 GB \\
        \hline
        3 & easyMKL [$K$] & 3 GB & 19 GB & 2 GB & 1 GB & 17 GB \\
        \hline
    \end{tabular}
    \caption{Memory occupation in GigaBytes for each dataset during the different
    experiments. Due to the parameter grid, in experiment 1 a total of 220 matrices
    where computed for each dataset (66 for GDD, Paragraph \ref{par:gdd}), while
    for experiments 2 and 3 a total of 110 matrices for each dataset where computed
    (33 for GDD).}
    \label{table:space}
\end{table}

Time resources requirements being a part of the proposed improvements will be given
in Section \ref{subsec:results}.
%----------------------------------------------------------------------------------------

\section{Baselines}
To establish a benchmark we compared our methodology with the results we got
from testing some baseline performances against the same datasets.
Table \ref{table:baselines} describes the general structure of the experiments
designed with this aim.
Moreover, for the first combination of the list ($ODDK_{ST}$ and $TCK_{ST}$)
we compared our results against those in \cite{gmkl}.

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|l|}
        \hline
        exp. no. & method & kernels \\
        \hline
        4 & easyMKL & $K$ and $K'$ \\
        \hline
        5 & easyMKL & $K'$ \\
        \hline
        6 & easyMKL & $K$ \\
        \hline
        7 & SVC & $K'$ \\
        \hline
        8 & SVC & $K'+K$ \\
        \hline
    \end{tabular}
    \caption{Structure of the baseline experiments. Kernels $K$ and $K'$ refer
    to the version without and with contexts for each combination respectively.
    These kernels generated a set of matrices each, according to the hyper-parameters
    grid, which were concatenated as a list to be given in input to easyMKL or
    given individually to an instance of the SVC. The expression $K + K'$ refers
    to the fact that the two kernel were summed prior to be used.}
    \label{table:baselines}
\end{table}

\subsubsection{easyMKL baselines}
These baselines basically replicates the three main experiments described in Section
\ref{sec:description} but without the features subdivision strategy described in
Section \ref{subsec:features}.
For the kernels derived from the $ODD$ kernels we employed a parameters grid composed
of the sets $h=\{1,\dots,10\}$ and $\lambda=\{0.1, 0.5, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.8\}$,
both sets were taken from \cite{rtesselli}.
The kernels derived from the $WL$ framework had their only hyper-parameter validated
in the set $h=\{1,\dots,10\}$.
During the cross-validation routine, the $\Lambda$ parameter of easyMKL has been
validated from the set $\{0.0, 0.1,\dots,1.0\}$.

\subsubsection{Baselines with the SVM classifier}

Each one of the kernel computed for experiment 5 (Table \ref{table:baselines}) were
individually used to train a SVM classifier whose $C$ parameter was validated
in the set $\{10^{-4},10^{-3},\dots,10^3\}$ \cite{rtesselli}.

%----------------------------------------------------------------------------------------

\section{Results and discussion}
\label{sec:results}

\subsection{Analysis of the computational times}
\label{subsec:time_results}

We now present an analysis of the computational time performances of our methodology
compared to the two baselines. To keep the results comparable we are here considering
only the results obtained by experiments 2, 6, and 8 (cfr. Table \ref{table:structure} and \ref{table:baselines});
this choice stem from the fact
that these experiments employ the same kernel function with three different methods.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{Figures/3datasetstimes}
    \caption{(DRAFT: this plot will be done ex-novo and replicated for each one of the kernels combination)
        plot of the times require to compute a full nested
        10-fold cross-validation on the shown datasets.
        The data shown here refers to experiments 2 (blue), 6 (orange) and 8 (yellow).
        (DRAFT: partial results from the other two datasets seem to agree with this plot)
    }
        \label{fig:datasetstimes}
\end{figure}

As one can see from Figure \ref{fig:datasetstimes}, our method performs generally
better or has comparable performances with respect to the single-kernel approach.
The only case where our method fares worse is with the GDD dataset and this can be
ascribed to the particular case which it represents, that is, due to the parameters
set limitation in this case we had way less matrices than we should have had and
this positively affects the single kernel method while being of no relevance for
easyMKL, way less influenced by the number of input matrices.
From this data we can finally conclude that our methodology is more susceptible
to dataset dimension variation (i.e. the number of records) while the single-kernel
method is ideed influenced by the same measure but it is even more influenced by
the dimensions of the parameters grid.

\subsection{Analysis of the models performances}

Here we present the empirical results we got from the devised experiments described
in the previous sections.
Data is divided in three tables, one for each of the three combinations we chose.
Each table reports the ROAUC measure with the standard deviation measured with
a nested 10-fold cross validation. (DRAFT: should we substitute it with a 95\% confindence interval?)
ROAUC was chosen over accuracy to be able to compare the results obtained in \cite{gmkl}.

Looking at Table \ref{table:results_st}, it is immediately evident how the results
obtained by our methodology are alwasy comparable when not even better with respect
to the baselines except in when considering the GDD dataset which, as mentioned in
Section \ref{subsec:time_results} is a rather peculiar case given the smaller dimensions
of the parameters grid considered.
While the results for experiment 2 seem in accordance with the results in \cite{gmkl},
the $TCK_{ST}$ (experiment 3) performs significantly worse with our $MKL$ method than with
the $SVM$.
(DRAFT: experiments 4 and 6 have unexpected high performancs though.)

\begin{landscape}
%    \begin{table}[ht]
%        \label{table:times}
%        \caption{(DRAFT) Times}
%    \end{table}

    \begin{table}[ht]
        \begin{tabular}{|l|l|r|r|r|r|r|}
            \hline
            n. &method&CAS&NCI1&AIDS&CPDB&GDD\\
            \hline
            1& $MKL~(ODDK_{ST}, TCK_{ST})^*$&&&0.8632 $\pm$ 0.0034&\textbf{0.8632 $\pm$ 0.0038}&0.8528 $\pm$ 0.0022\\
            2& $MKL~(ODDK_{ST})^*$&&&0.8627 $\pm$ 0.0035&\textbf{0.8632 $\pm$  0.0029}&0.8543 $\pm$ 0.0024\\
            3& $MKL~(TCK_{ST})^*$&&&\textbf{0.8634 $\pm$ 0.0034}&0.8625 $\pm$ 0.0032&0.8458 $\pm$ 0.0021\\
            \hline
            4& $MKL~(ODDK_{ST}, TCK_{ST})$&0.8960 $\pm$  0.0012&0.9076 $\pm$ 0.0007&0.8468 $\pm$ 0.0042&0.8517 $\pm$ 0.0034&0.8612 $\pm$ 0.0018\\
            5& $MKL~(ODDK_{ST})$&&0.8976 $\pm$ 0.0010 &0.8388 $\pm$ 0.0044&0.8401 $\pm$ 0.0033&0.8013 $\pm$ 0.0019\\
            6& $MKL~(TCK_{ST})$&0.8954 $\pm$ 0.0013&0.9095 $\pm$ 0.0006&0.8487 $\pm$ 0.0040&0.8525 $\pm$ 0.0030&0.8617 $\pm$ 0.0022\\
             & $MKL~(ODDK_{ST})^{**}$&\textbf{0.9049 $\pm$ 0.0008}&0.9144 $\pm$ 0.0008&0.8515 $\pm$ 0.0031&0.8564 $\pm$ 0.0056&0.8498 $\pm$ 0.0026\\
             & $SVM~(ODDK_{ST})^{**}$&0.8982 $\pm$ 0.0017&0.9069 $\pm$ 0.0010&0.8262 $\pm$ 0.0052&0.8442 $\pm$ 0.0067&0.8473 $\pm$ 0.0038\\
            7& $SVM~(TCK_{ST} + ODDK_{ST})$&0.9010 $\pm$ 0.0011&0.9110 $\pm$ 0.0011&0.8323 $\pm$ 0.0065&0.8497 $\pm$ 0.0072&0.8627 $\pm$ 0.0018\\
            8& $SVM~(TCK_{ST})$&0.9006 $\pm$ 0.0013&\textbf{0.9150 $\pm$ 0.0011}&0.8225 $\pm$ 0.0067&0.8422 $\pm$ 0.0080&\textbf{0.8674 $\pm$ 0.0026}\\
            \hline
        \end{tabular}
        \caption{ROAUC results ($\pm$ standard deviation) relative to the combination
            of the $ODD_{ST}$ kernel with the $TCK_{ST}$ kernel. Results are
            obtained from a nested 10-fold cross validation. The first column is
            given as a reference to the experiment description given in Section
            \ref{sec:description}.
            The top part of the table contains the results of the main experiments
            while the bottom part those of the baselines.
            Lines marked with $^*$ refer to kernels whose feature space was split
            according to the technique exposed in \ref{subsec:features}.
            Lines marked with $^{**}$ refer to results obtained in \cite{gmkl}.
        }
        \label{table:results_st}
        \medskip

        \begin{tabular}{|l|l|r|r|r|r|r|}
            \hline
            n. & metodo&CAS&NCI1&AIDS&CPDB&GDD\\
            \hline
            1& $MKL (ODDK_{ST+}, TCK_{ST+})^*$&&&0.8632 $\pm$  0.0034&0.8632 $\pm$  0.0038&0.8528 $\pm$ 0.0022\\
            2& $MKL (ODDK_{ST+})^*$&&&0.8628 $\pm$  0.0036&0.8652 $\pm$ 0.0030&\textbf{0.8720 $\pm$ 0.0021}\\
            3& $MKL (TCK_{ST+})^*$&&&\textbf{0.8649 $\pm$  0.0030}&\textbf{0.8666 $\pm$  0.0038}&0.8711 $\pm$ 0.0018 \\
            \hline
            4& $MKL (ODDK_{ST+}, TCK_{ST+})$&&&0.8468 $\pm$ 0.0042&0.8517 $\pm$ 0.0034&0.8612 $\pm$ 0.0018\\
            5& $MKL (ODDK_{ST+})$&&&0.8489 $\pm$  0.0039&0.8461 $\pm$ 0.0036&0.8178 $\pm$ 0.0022\\
            6& $MKL (TCK_{ST+})$&&&0.8503 $\pm$  0.0038&0.8528 $\pm$ 0.0039&0.8645 $\pm$ 0.0018\\
            7& $SVM (TCK_{ST+} + ODDK_{ST+})$&0.9022 $\pm$ 0.0015 &0.9163 $\pm$ 0.0011&0.8256 $\pm$ 0.0068&0.8521 $\pm$ 0.0038&0.8570 $\pm$ 0.0043\\
            8& $SVM (TCK_{ST+})$&0.9008 $\pm$ 0.0017&0.9165 $\pm$ 0.0013&0.8222 $\pm$ 0.0067&0.8462 $\pm$ 0.0048&0.8588 $\pm$ 0.0028\\
            \hline
        \end{tabular}
        \caption{ROAUC results ($\pm$ standard deviation) relative to the combination
                of the $ODD_{ST+}$ kernel with the $TCK_{ST+}$ kernel. Results are
                obtained from a nested 10-fold cross validation. The nomenclature and
            results disposition is analogue to the one of Table \ref{table:results_st}.}
        \label{table:results_stp}
    \end{table}

%    \begin{table}[ht]
%        \label{table:results_wl}
%        \caption{ROAUC results ($\pm$ standard deviation) relative to the combination
%                of the $WL$ kernel with the $WLC$ kernel. Results are
%                obtained from a nested 10-fold cross validation. The nomenclature and
%            results disposition is analogue to the one of Table \ref{table:results_st}.}
%        \label{table:results_wl}
%    \end{table}
\end{landscape}

\section{Kernels Contribution Analysis}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{Figures/weightdist}
    \caption{(DRAFT: this plot is just an example of the available data for furhter analisys.
        I think we may be mainly interested in the experiments were two kernels are combined
        either with or without bucketization of the feature space.)
        Kernel weight distributions for the $TCK_{ST}$ kernel
    computed according to the full parameters grid, on the NCI1 dataset.
    The $x$ axis shows all the employed kernel matrices indexed by their hyperparameters
    values (ascending values left to right), the $y$ axis the weight values.
    Each plot refers to one run of the nested 10-fold cross-validation routine relative to one
    value for the $\Lambda$ parameter of easyMKL. Weight values are the means between each of the inner
    cross-validation runs. The vertical lines highlight the kernel with the maximum weight for each value of $\Lambda$.}
        \label{fig:weightdist}
\end{figure}


