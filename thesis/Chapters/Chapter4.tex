% Chapter 4

\chapter{Experiments}
\label{Chapter4}

In order to assess the quality of the proposed methodology we tested it on a
number of datasets and against a number of baselines that will be detailed in
the following sections.
Since our methodology has the parameter selection phase embedded in the learning
process (section~\ref{subsec:parameters}) the experiments structure were quite
straightforward, namely consisting in the pre-calculation of the relevant kernel
matrices, and then using them all at once in a stratified k-fold cross-validation
routine with the learning method of choice.
The $k$ parameter of the cross-validation has been fixed at 10 since this value
already provides good statistical significance while helping containing the bias
skewing during the training phase.
Moreover, the whole routine has been run ten times with ten different split of the
data to mitigate the high variance derived during the testing phase due to the
chosen value of $k$.

\section{Datasets}
\label{subsec:datasets}

The experiments were conducted on five real-world applications datasets, namely:
CAS \cite{cas}, NCI1 \cite{nci1}, AIDS \cite{aids}, CPDB \cite{cpdb}, and GDD \cite{gdd}.
All these datasets contains chemical and molecular particles encoded in graph form.
The AIDS Antiviral Screen dataset contains chemical compounds, categorized
into chemical groups; CAS and CPDB are dataset of mutagenic
compounds; NCI1 consists of chemical compounds screened for activity against 
non-small lung cancer cells; GDD is composed of X-ray crystal structures of
proteins represented as graphs.
The graph encoding of this data is such that every node is labelled and none
has a self loop that is there are no nodes with an edge going out and going back
into them.
Some statistics about these datasets are gathered in table~\ref{table:datasets}.

    \begin{table}[ht]
        \centering
        \begin{tabular}{|r|r|r|r|r|}
            \hline
            Dataset & n. of graphs & sample split & avg nodes & avg edges \\ \hline
            AIDS    & 1503         & 28.07        & 58.90     & 61.40     \\ \hline      
            CAS     & \textbf{4337} & 55.36        & 29.90     & 30.90     \\ \hline      
            CPDB    &  684         & 49.85        & 14.10     & 14.60     \\ \hline      
            GDD     & 1503         & 58.65        & 284.31    & \textbf{2862.63}   \\ \hline      
            NCI1    & 1503         & 50.04        & 29.87     & 32.30     \\ \hline      
        \end{tabular}
        \caption{Statistics about the datasets employed in the experiments: number
        of graphs, labels percentage among samples, average number of nodes, average
        number of edges. It is clear from this table that the CAS and NCI1 datasets
        are the bigger ones while the GDD holds the most complex graphs either in
        terms of topography and processing. Furthermore the AIDS dataset turns out
        to be quite unbalanced in terms of labels distribution \cite{rtesselli}.}
        \label{table:datasets}
    \end{table}

%----------------------------------------------------------------------------------------

\section{Experiments description}
\label{sec:description}

The original kernel combinations selected for this study were:
\begin{enumerate}
    \item the $ODDK_{ST}$ and $TCK_{ST}$ graph kernels,
    \item the $ODDK_{ST+}$ and $TCK_{ST+}$ graph kernels,
    \item the $WL$ fast subtree and $WLC$ graph kernels,
\end{enumerate}

for each combination a separated set of experiment was conducted, while maintaining
the same structure, of which a detailed breakdown will be given in Table \ref{table:structure}.

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|l|}
        \hline
        exp. no. & method & kernels \\
        \hline
        1 & easyMKL & $K$ and $K'$ \\
        \hline
        2 & easyMKL & $K'$ \\
        \hline
        3 & easyMKL & $K$ \\
        \hline
    \end{tabular}
    \caption{Structure of the main experiments. Kernels $K$ and $K'$ refer
    to the version without and with contexts for each combination respectively.
    These kernels generated a set of matrices each, according to the technique
    described in Section \ref{subsec:features} which were concatenated as a list
    to be given in input to easyMKL.}
    \label{table:structure}
\end{table}

The hyper-parameter selection process for the kernels has been embedded
into the MKL learning phase so each set of kernel matrices has been
pre-computed for each one of the possible combination of parameters that would
otherwise have been used in a grid-search fashion.

\subsubsection[$ODDK_{ST}$ and $TCK_{ST}$]{$\boldsymbol{ODDK_{ST}}$ and $\boldsymbol{TCK_{ST}}$}
The kernels used for this experiment were generated from the $ODDK$ kernels
in \cite{odd, contexts}, namely $ODDK_{ST}$, $TCK_{ST}$.
Since EasyMKL took care of weighing the individual kernels, the $\lambda$
parameters of the ODD kernels has been fixed to 1, while the $h$ parameter
values were the set $\{1,\dots,10\}$.
During the cross-validation routine, the $\Lambda$ parameter of easyMKL has been
validated from the set $\{0.0, 0.1,\dots,1.0\}$.

\subsubsection[$ODDK_{ST+}$ and $TCK_{ST+}$]{$\boldsymbol{ODDK_{ST+}}$ and $\boldsymbol{TCK_{ST+}}$}
The kernels used for this experiment were generated from the $ODDK$ kernels
in \cite{stplus, rtesselli}, namely $ODDK_{ST+}$, $TCK_{ST+}$.
Again, EasyMKL took care of weighing the individual kernels so the $\lambda$
parameters of the ODD kernels has been fixed to 1, while the $h$ parameter
values were the set $\{1,\dots,10\}$.
During the cross-validation routine, the $\Lambda$ parameter of easyMKL has been
validated from the set $\{0.0, 0.1,\dots,1.0\}$.

\subsubsection[$WL$ and $WLC$]{$\boldsymbol{WL}$ and $\boldsymbol{WLC}$}
The kernels used for this experiment were generated from the $Fast~Subtree$ kernels
in \cite{fs, rtesselli}, namely the $WL$ kernel and the $WLC$ kernel.
These two kernels only need one hyper-parameter to validate, that is $h$ the iterations
limit whose chosen values were those in the set $\{1,\dots,10\}$.
During the cross-validation routine, the $\Lambda$ parameter of easyMKL has been
validated from the set $\{0.0, 0.1,\dots,1.0\}$.

\subsubsection{The GDD dataset}
\label{subsubsec:gdd}
Confirming the experience reported in \cite{contexts}, while working with the GDD
dataset and the $ODD$ kernels we had to limit the $h$ hyper-parameter to the set
$\{1,2,3\}$ because for heights greater than 3 the computational times of the
kernel matrices became prohibitive, due to the high complexity and magnitude
of the data structures contained in this dataset.

\subsection{Space resources requirements}
Given the large number of kernel matrices involved in the combination exepriments
we would like to detail the space resources requirements that these experiments
had (Table \ref{table:space}) with our setup.

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|}
        \hline
        exp. no. & method [kernel(s)] & AIDS & CAS & CPDB & GDD & NCI1 \\
        \hline
        1 & easyMKL [$K$ and $K'$] & 5 GB & 34 GB & 3 GB & 3 GB & 29 GB \\
        \hline
        2 & easyMKL [$K'$] & 3 GB & 19 GB & 2 GB & 1 GB & 17 GB \\
        \hline
        3 & easyMKL [$K$] & 3 GB & 19 GB & 2 GB & 1 GB & 17 GB \\
        \hline
    \end{tabular}
    \caption{Memory occupation in GigaBytes for each dataset during the different
    experiments. Due to the parameter grid, in experiment 1 a total of 220 matrices
    where computed for each dataset (66 for GDD, Section \ref{subsubsec:gdd}), while
    for experiments 2 and 3 a total of 110 matrices for each dataset where computed
    (33 for GDD).}
    \label{table:space}
\end{table}

Time resources requirements being a part of the proposed improvements will be given
in Section \ref{subsec:results}.
%----------------------------------------------------------------------------------------

\section{Baselines}
To establish a benchmark we compared our methodology with the results we got
from testing some baseline performances against the same datasets.
Table \ref{table:baselines} describes the general structure of the experiments
designed with this aim.

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|l|}
        \hline
        exp. no. & method & kernels \\
        \hline
        4 & easyMKL & $K$ and $K'$ \\
        \hline
        5 & easyMKL & $K'$ \\
        \hline
        6 & easyMKL & $K$ \\
        \hline
        7 & SVC & $K'$ \\
        \hline
        8 & SVC & $K'+K$ \\
        \hline
    \end{tabular}
    \caption{Structure of the baseline experiments. Kernels $K$ and $K'$ refer
    to the version without and with contexts for each combination respectively.
    These kernels generated a set of matrices each, according to the hyper-parameters
    grid, which were concatenated as a list to be given in input to easyMKL or
    given individually to an instance of the SVC. The expression $K + K'$ refers
    to the fact that the two kernel were summed prior to be used.}
    \label{table:baselines}
\end{table}

\subsubsection{easyMKL baselines}
These baselines basically replicates the three main experiments described in Section
\ref{sec:description} but without the features subdivision strategy described in
Section \ref{subsec:features}.
For the kernels derived from the $ODD$ kernels we employed a parameters grid composed
of the sets $h=\{1,\dots,10\}$ and $\lambda=\{0.1, 0.5, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.8\}$,
both sets were taken from \cite{rtesselli}.
The kernels derived from the $WL$ framework had their only hyper-parameter validated
in the set $h=\{1,\dots,10\}$.
During the cross-validation routine, the $\Lambda$ parameter of easyMKL has been
validated from the set $\{0.0, 0.1,\dots,1.0\}$.

\subsubsection{Baselines with the SVM classifier}

Each one of the kernel computed for experiment 5 (Table \ref{table:baselines}) were
individually used to train a SVM classifier whose $C$ parameter was validated
in the set $\{10^{-4},10^{-3},\dots,10^3\}$ \cite{rtesselli}.

%----------------------------------------------------------------------------------------

\section{Results and discussion}
\label{subsec:results}

\subsection{Analysis of the computational times}

\subsection{Analysis of the models performances}

\begin{landscape}
    \begin{table}[ht]
        \centering
        \begin{tabular}{|l|r|r|r|r|r|}
            \hline
            method&CAS&NCI1&AIDS&CPDB&GDD\\
            \hline
            MKL (ODD\_ST, TCK\_ST)* &&&0.8632 $\pm$ 0.0034&0.8632 $\pm$ 0.0033&\\
            \hline
            MKL (ODD\_ST)*&&&&&\\
            \hline
            MKL (TCK\_ST)*&&&&&\\
            \hline
            MKL (ODDS\_ST, TCK\_ST)&&&0.8468 $\pm$ 0.0041&0.8517 $\pm$ 0.0033&0.8611 $\pm$ 0.0017\\
            \hline
            MKL (ODD\_ST)&&&0.8387 $\pm$ 0.0043&0.8400 $\pm$ 0.0032&0.8012$\pm$ 0.0018\\
            \hline
            MKL (TCK\_ST)&0.8953 $\pm$ 0.0013&0.9095 $\pm$ 0.0006&0.8486 $\pm$ 0.0040&0.8525 $\pm$ 0.0029&0.8617 $\pm$ 0.0021\\
            \hline
            MKL (MKL-ODD)** &0.9049 $\pm$ 0.0008&0.9144 $\pm$ 0.0008&0.8515 $\pm$ 0.0031&0.8564 $\pm$ 0.0056&0.8498 $\pm$ 0.0026\\
            \hline
            SVM (ODD\_ST)** &0.8982 $\pm$ 0.0017&0.9069 $\pm$ 0.0010&0.8262 $\pm$ 0.0052&0.8442 $\pm$ 0.0067&0.8473 $\pm$ 0.0038\\
            \hline
            SVM (TCK\_ST +ODD\_ST)&0.9009 $\pm$ 0.0010&0.9110 $\pm$ 0.0011&0.8322 $\pm$ 0.0065&0.8497 $\pm$ 0.0072&0.8626 $\pm$ 0.0018\\
            \hline
            SVM (TCK\_ST)&0.9006 $\pm$ 0.0013&0.9149 $\pm$ 0.0010&0.8225 $\pm$ 0.0066&0.8421 $\pm$ 0.0080&0.8674 $\pm$ 0.0026\\
            \hline
        \end{tabular}
        \label{table:results_st}
        \caption{ROAUC measure relative to the first group of experiments.}

        \centering
        \begin{tabular}{|l|r|r|r|r|r|}
            \hline
            method&CAS&NCI1&AIDS&CPDB&GDD\\
            \hline
            MKL ($ODD_{ST+}, TCK_{ST+}$)*&&&0.8632 $\pm$ 0.0034&0.8632 $\pm$ 0.0033&\\
            \hline
            ODD\_ST con MKL*&&&&&\\
            \hline
            TCK\_ST con MKL*&&&&&\\
            \hline
            TCK\_ST (+ODD\_ST) con SVM&0.9009 $\pm$ 0.0010&0.9110 $\pm$ 0.0011&0.8322 $\pm$ 0.0065&0.8497 $\pm$ 0.0072&0.8626 $\pm$ 0.0018\\
            \hline
            TCK\_ST con SVM&0.9006 $\pm$ 0.0013&0.9149 $\pm$ 0.0010&0.8225 $\pm$ 0.0066&0.8421 $\pm$ 0.0080&0.8674 $\pm$ 0.0026\\
            \hline
            ODDS\_ST, TCK\_ST con MKL&&&0.8468 $\pm$ 0.0041&0.8517 $\pm$ 0.0033&0.8611 $\pm$ 0.0017\\
            \hline
            ODD\_ST con MKL&&&0.8387 $\pm$ 0.0043&0.8400 $\pm$ 0.0032&0.8012$\pm$ 0.0018\\
            \hline
            TCK\_ST con MKL&0.8953 $\pm$ 0.0013&0.9095 $\pm$ 0.0006&0.8486 $\pm$ 0.0040&0.8525 $\pm$ 0.0029&0.8617 $\pm$ 0.0021\\
            \hline
        \end{tabular}
        \label{table:results_stp}
        \caption{ROAUC measure relative to the second group of experiments.}
    \end{table}

    \begin{table}[ht]
        \centering
        \begin{tabular}{|l|r|r|r|r|r|}
            \hline
            method&CAS&NCI1&AIDS&CPDB&GDD\\
            \hline
            ODD\_ST, TCK\_ST con MKL*&&&0.8632 $\pm$ 0.0034&0.8632 $\pm$ 0.0033&\\
            \hline
            ODD\_ST con MKL*&&&&&\\
            \hline
            TCK\_ST con MKL*&&&&&\\
            \hline
            TCK\_ST (+ODD\_ST) con SVM&0.9009 $\pm$ 0.0010&0.9110 $\pm$ 0.0011&0.8322 $\pm$ 0.0065&0.8497 $\pm$ 0.0072&0.8626 $\pm$ 0.0018\\
            \hline
            TCK\_ST con SVM&0.9006 $\pm$ 0.0013&0.9149 $\pm$ 0.0010&0.8225 $\pm$ 0.0066&0.8421 $\pm$ 0.0080&0.8674 $\pm$ 0.0026\\
            \hline
            ODDS\_ST, TCK\_ST con MKL&&&0.8468 $\pm$ 0.0041&0.8517 $\pm$ 0.0033&0.8611 $\pm$ 0.0017\\
            \hline
            ODD\_ST con MKL&&&0.8387 $\pm$ 0.0043&0.8400 $\pm$ 0.0032&0.8012$\pm$ 0.0018\\
            \hline
            TCK\_ST con MKL&0.8953 $\pm$ 0.0013&0.9095 $\pm$ 0.0006&0.8486 $\pm$ 0.0040&0.8525 $\pm$ 0.0029&0.8617 $\pm$ 0.0021\\
            \hline
        \end{tabular}
        \label{table:results_wl}
        \caption{ROAUC measure relative to the third group of experiments. }
    \end{table}
\end{landscape}

\section{Kernels Contribution Analysis}
\dots eventuale plot dei pesi delle matrici per gli esperimenti principali

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{Figures/weightdist}
    \caption{Kernel weight distribution for the $TCK_{ST}$ kernel
    computed according to the full parameters grid.
    The $x$ axis shows the kernel indexed by their hyperparameters values,
    the $y$ axis the weight values.
    Each plot refers to one run of the cross-validation routine relative to one
    value for the $\Lambda$ parameter of easyMKL. The vertical lines highlight
    the kernel with the maximum weight for each value of $\Lambda$.}
        \label{fig:weightdist}
\end{figure}


