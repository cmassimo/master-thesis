% Chapter 2

\chapter{Background} % Main chapter title
This chapter covers in more detail the fundamental concepts needed to assure a
thorough comprehension of the topics discussed in the following chapters.
We start by giving a definition of machine learning, analysing the 
principal aspects of the general approach such as risk minimization and evaluation
strategy.
Then Section \ref{sec:kernel} deals with the theoretical foundations of the 
methods that are in the scope of this thesis, while Section \ref{sec:graphkernels}
delves in the details of the various approaches that will be covered.

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter2} 

\section{Machine Learning}

Machine learning is the Artificial Intelligence branch discipline that aims
to approximate human learning processes, which are still rather obscure,
by the means of algorithms and formal methods.
Central to this discipline is the idea of learning a \emph{concept}, or a function
$c(x_i)$ on some example data $x_i \in X$, where the learning process consist
mainly in finding an approximation good enough for the task at hand.
The learning process is carried on by an algorithm which builds an hypothesis
on the available data and uses that hypothesis, let's say $h()$,  to approximate
$c()$.
One of the main goal of such an algorithm is to use the given data to constantly
improve the hypothesis $h()$ up to a certain performance measure, depending
on the task, the paradigm considered or other external factors such as available
resources.

This approach is better exploited in scenarios where either an exact
algorithmic solution to the problem (learning $c()$) would be infeasible, when
there is too much uncertainty in the data or when the problem definition itself
is too difficult to formally define or too vague.
Since a machine learning algorithm is often looking for an approximation of the 
optimal solution, the first problem can be solved settling with a suboptimal
solution that is more easily obtainable; given the iterative fashion in which
the learning progress, data noise can be recognised and avoided; lastly,
an algorithm that learns from the data can be used to explore multiple problem
definitions, as is often the case with more \emph{data mining} oriented approaches.

One of the most common examples of machine learning applications is the binary
classification of so-called spam emails.
This problem states that an email can either be ``spam'' or not but the definition
of ``being spam'' is not unique or universally accepted, so we need to be guided
in the initial definition by a human counterpart: we submit a set of emails to a
human supervisor that labels them for us then, from this initial evidence, a
machine learning algorithm tries to ``learn'' this particular concept of spam
and uses this knowledge to label future incoming emails which if confirmed by the
supervisor go to increase the algorithm's knowledge and possibly meliorate it in
a continuing iterative fashion.

This iterative process of learning from experience, incorporating the new one
as it comes, trying to make better decision based on a performance measure, is
the fundamental scheme behind the learning process in machine learning and can
be summed by the following definition:

\begin{definition}[Learning]
    A computer program is said to learn from experience \emph{E} with
    respect to some class of tasks \emph{T} and performance measure \emph{P},
    if its performance at tasks in \emph{T}, as measured by \emph{P}, improves
    with experience \emph{E} [Mitchell].
\end{definition}

Learning in this context can be divided in three main paradigms, of which this
thesis covers only the first:
\begin{description}
    \item [Supervised learning:] the function to learn has the shape
    $c: X \to Y$ for some sample data $X$ and some target value $Y$.
    The supervision comes from the fact that the target function values are
    given from above by an ``expert'' and plays an active guidance role throughout
    the whole learning process.
    \item [Unsupervised learning:] in this case no target function values are
        given so the algorithm works ``blindly'' trying to catch patterns or
        regularities coming only from the data itself.
    \item [Reinforcement learning:] this is a form of supervised learning
        where the algorithm gets a reward at each search step, reward that can
        be either positive, negative or null, and tries to find the optimal
        strategy to maximize the total reward.
\end{description}

Another big dichotomy in machine learning is given by the way we acquire the
available data, of which again this thesis covers only the first one:

\begin{description}
    \item [Batch learning:] all the data available for learning is given to the
        algorithm beforehand and it all concur, one way or another, in the
        learning process.
    \item [Online learning:] the data becomes available as it gets produced
        and collected so the algorithm can base its learning process only
        on the currently observed sample.
\end{description}

\subsection{Supervised Learning}

This work uses a supervised learning approach, in that the dataset
used in the experiments consists in previously and correctly labelled records with
reasonable certainty.
We are then facing a \emph{classification} problem, one in which we need to learn
a decision function that will classify each new record with the correct label.
In this paradigm we are given a set of tuples (i.e. examples) called \emph{training set}:
$S = \{(x_i, y_i)| i=1,\dots,n\}$ with $x_i \in \mathrm{X}, y_i \in \mathrm{Y}$
where $\mathrm{X}$ is the set of data instances and $\mathrm{Y}$ is the set of
labels.
The set $S$ is governed by an underlying unknown probability distribution $P$ over
$\mathrm{X}$.
Furthermore, the domain of $\mathrm{Y}$ defines the type of classification task
that the algorithm should perform:
\begin{itemize}
    \item $Y = \{\pm{1}\}$: binary classification
    \item $Y = \{1,\dots,n\}$: multi-class classification
    \item $Y = \mathbb{R}$: regression, or the approximation of a real function
\end{itemize}

The current work will focus on the first of the three, in other words we will
try to approximate as tightly as possible the function $c:X\to Y$ with an
hypothesis $h:X\to Y, h\in \mathcal{H}$, where $\mathcal{H}$ is the hypotheses
space that is fixed a priori, setting an inductive bias.
Bias can be introduced either by said choice of the hypotheses space or by the
choice of the learning algorithm and is a guarantee that learning is taking place:
a infinite hypotheses space and an exhaustive search algorithm would render
any learning useless since the algorithm could return an infinite number of
hypothesis from $\mathcal{H}$ that could fit any given data.

\subsection{Overfitting}
The kind of bias that we apply to our learning process affects the resulting
hypothesis in ways that can be counter-productive.
Generally is assumed that the data in the training set is generated according
to an unknown probability distribution $P$ on $X$.
We aim to find an optimal hypothesis $h^*$ that is, the one that minimizes the
prediction error (often referred to as \emph{risk} or ideal error) which is
defined as:

\[R(h)=\int_{X\times Y} L(h(x),y)~dP(x,y)\]

where $L$ is a loss function; given that $P$ is unknown it follows that also
$R(h)$ for any given $h$ is unknown, we can only give a bound on it and measure
the loss function on the classified data we already have, with the following
formula:

\[R_e(h)=\frac{1}{N} \sum_{(x,y) \in S} L(h(x),y)\]

While minimizing the empirical error $R_e(h)$ might be tempting, doing so will
inevitably tailor our hypothesis to the data we have in our training set which
tells us nothing about the underlying (unknown) distribution $P$ nor about the 
possibly infinite set of data that $h$ might have to classify.
Hence we are not consistently modelling the decision function that we are trying
to learn but just the one that fits the set of samples.
Empirical error minimization alone will eventually result in increasing the ideal
error because one of the effect of the minimization is that $h$ will lack
\emph{generalization}.
This situation is called \emph{overfitting} and it occurs when the either the
hypotheses space is too complex or the model has too many parameters with
respect to the number of samples i.e. we are trying too hard to model the
available data.
The contrary is called \emph{underfitting} and negatively affects the
generalization power of an hypothesis by making it fail to learn much from the
sample data.

\subsubsection{Structural Risk Minimization}
\label{subsubsec:srm}
To avoid the scenarios depicted in the previous section we need a way to determine
how powerful an hypothesis can be, i.e. its expressiveness.
This is achieved by measuring the complexity of the originating hypotheses space,
using a measure called \emph{Vapnik-Chervonenkis dimension} (VC-dimension).
First we will introduce the concept of shattering of a set $\mathrm{X}$:
\begin{definition}[Shattering]
    $S \subset X$ is shattered by an hypotheses space $\mathcal{H}$ iff
    \[\forall S' \subseteq S, \exists h \in \mathcal{H} s.t. \forall x \in S, h(x)=1 \iff x \in S'\]
\end{definition}
i.e. $\mathcal{H}$ implements all the possible dichotomies of $S$.
Given this definition we can now proceed to define more formally what the
VC-dimension is:
\begin{definition}[VC-dimension]
    The VC-dimension of an hypotheses space $\mathcal{H}$, defined on an sample
    space $\mathrm{X}$, is the cardinality of the largest subset of $\mathrm{X}$
    shattered by $\mathcal{H}$, or:
    \[VC(\mathcal{H}) = \max_{S\subseteq \mathrm{X}} |S| : \mathcal{H}\text{ shatters }S\]
\end{definition}
and indeed $VC(\mathcal{H})=\infty$ if $S$ is infinite.

Now, while referring to a binary classification task as is the scope of the present
work, we can show how the VC-dimension affects the bound on the ideal error:
\begin{displaymath}
    R_D(h_{w^*}(x)) \leq R_e(h_{w^*}(x)) + \sqrt{\frac{VC(\mathcal{H})}{N}
    (\log{(\frac{2N}{VC(\mathcal{H})})}+1) - \frac{1}{N}\log{\delta}}
\end{displaymath}
where $R_D$ is the true risk (i.e. ideal error), $R_e$ is the empirical error,
$h_{w^*}(x)$ is the optimal hypothesis returned by the algorithm and $N$ is the
cardinality of the training set.

As we can see, the first term in the rightmost part of the inequality only 
depends from the hypothesis while the second term depends from the ratio
between the VC-dimension and training set size, beside the confidence ($\delta$)
with which the bound is valid.
This last term is generally called VC-confidence.

From these premises we can assert that selecting a complex hypotheses space,
that is one with a high VC-dimension, or whose hypotheses can successfully shatter
very large sets, will indeed make $R_e$ decrease since it will generate more
expressive hypotheses that will better fit the data; on the other hand the ratio
$\frac{VC(\mathcal{H})}{N}$ will also increase making the VC-confidence increase
as well, negatively affecting the overall ideal error bound.

A tried approach to balance this two terms is called \emph{structural risk
minimization} which considers hypotheses spaces of crescent complexity
(i.e. increasing VC-dimension) and for each one selects the hypothesis with
the lower empirical error, finding the hypothesis with the lower bound on the
ideal error.

\subsection{Hypothesis Evaluation Strategy}
\label{subsec:evaluation}

Given a dataset, we could train our algorithm an all the samples and that would
maximize the learning performance since the cardinality of the training set is
inversely proportional to the bound on the ideal error.
However we cannot rely on the learning phase alone but we also need a way to
assess the goodness of a given hypothesis with respect to a given metric, be it 
either the prediction accuracy or a more sophisticated one like the ROC AUC.
In this thesis a variant of the technique explained in the following section
has been employed.

%seeds + 10FCV + parameter selection embedded in the validation process
%The final results were obtained by combining a standard practice evaluation
%technique with the parameters selection process.
%Every experiment has been conducted employing the K-fold nested cross-validation
%technique [ref].

\subsubsection{Cross Validation}
\label{subsubsec:cv}
This technique is based on the minimization of the estimated ideal error by
(repeatedly) splitting the whole data set into two separate sets: a training set
and a test set.
Training is performed on the training set and the resulting hypothesis performance
is tested on the test set.
The point of this process is to simulate the situation in which the hypothesis
face prediction on previously completely unseen samples thus mimicking the real
world scenario.
A considerable drawback that needs to be weighed in during this phase is the 
possibly consistent reduction of the cardinality of the training set which as 
previously noted will likely lead to increase the bound on true risk.

Cross validation can be done in several ways. Typically the \emph{k-fold} cross
validation technique is employed, where $k$ is a fixed integer value that
determines the number of slices the training set will be split into, each one to
be used as the validation set during the relative fold, while the remaining $k-1$
become the new training set.
After $k$ folds have returned $k$ different hypothesis, the one with the best
performance on its validation set is returned as the best one.
The variation of $k$ can dramatically affect the overall learning outcome.
For $k=1$ the technique is called \emph{leave-one-out} and has the peculiarity
of decreasing the model bias (outliers will not affect the model) but increasing
the variance on the validation set.
On the other hand for large values of $k$, we obtain the opposite effect.

\subsubsection{Nested Cross Validation}
\label{subsubsec:ncv}
An important thing to note at this point is that with the technique explained in
the previous section we are not assessing the performance of each hypothesis
independently from the training set since every validation set becomes at some
point part of the training set of another fold.
This will likely lead to an overestimate of the hypothesis performance.

It is hence mandatory that the test set must be left out from the whole training
process and used exclusively for performance assessment purposes.
The scheme consists of two nested loops, in the first loop, the training set
is split into $k$ subsets, $k-1$ of which become the training set for the
$i^{th}$ iteration of training while the $k^{th}$ set becomes the test set.
This subset of the original training set is then split again in $k$ subsets
which again are used for training and validation with the usual cross validation
technique.
Once the best hypothesis has been selected in the innermost loop, it is trained
with the whole outer loop training set and tested against the test set, which has
remained completely isolated from the learning phase until now.
When used in combination with hyper-parameter optimization with the so called grid
search technique, the performances obtained in the outer loop are usually used
to determine the best set of parameters in what is called model selection.

\subsubsection{Grid Search}
\label{subsubsec:grid}
An often used technique to perform hyper-parameter optimization is the exhaustive
search on a hand-picked sub-space of the hyper-parameters space that is, selecting
a set of value for each parameter then evaluating every possible combination
deriving from the cartesian product of these sets by the means of cross-validation.
This approach is potentially doomed by the curse of dimensionality, a solution
for which could be a limited randomized search, but is also clearly very easily
parallelizable.

%----------------------------------------------------------------------------------------

\section{Kernel Methods}
\label{sec:kernel}

In this section we briefly analyse a family of methods that rely on a solid
theoretical framework.
\emph{Kernel methods} collect all those techniques that represent the hypothesis
in terms of the input samples.
These methods do not work on the explicit representation of the examples but
need just a measure of their pairwise similarity.
For the whole thing to be sound this measure has to be computed using a
\emph{kernel function}.

The two main components of a kernel method are:
\begin{itemize}
    \item a problem-specific kernel function
    \item a general purpose learning algorithm
\end{itemize}
now we will give a more detailed overview of these two main concepts.

\subsection{Kernel functions}
\label{subsec:kernelfunc}
A function $K:X\times X \to Y$ is a kernel function if it satisfies the following properties:
\begin{itemize}
    \item it is a continuous function
    \item it is symmetric i.e. $K(x,y) = K(y,x)~\forall x,y \in X$
    \item it is positive-semidefinite that is, if $\forall N\geq 1, \forall x_1,\dots x_N \in X$,
        the matrix defined as $K_{i,j} = K(x_i,x_j)$ is positive-semidefinite,
        or $\sum_{i,j}c_ic_jK_{i,j}~\geq~0$ $\forall c_1,\dots c_N \in \mathbb{R}$
        or equivalently if all its eigenvalues are non-negative.
\end{itemize}

Being able to represent each sample $x \in X$ as $\phi(x) = \{\phi_n(x)\}_{n \geq 1}$
so that computing $K(x,y)$ is equivalent to computing the dot product $\langle
\phi(x),\phi(y)\rangle$ then $K$ is a kernel.
The converse is always true when $X$ is a countable set for an opportune choice
of $\phi$.
The vector space defined by such $\phi$ is called \emph{feature space}.

A useful extension, called \emph{zero extension}, is available given a kernel $K$
defined on a certain input space, say $S\subset X$; we can extend it to work
on $X\times X$ thanks to it being positive-semidefinite i.e.
$K(x,y)=0~\forall x,y \in X\setminus S$.

Kernel are also demonstrably closed under the sum operation:
\begin{theorem}
    Given two valid kernels $K_1(x,x')$ and $K_2(x,x')$, then $c_1K_1(x,x') +
    c_2K_2(x,x'), c_1,c_2 \geq 0$, is a valid kernel.
\end{theorem}
\begin{proof}
    Given any final set of instances $\{x_1,\dots,x+n\}$, let $K_1$ and $K_2$ be
    the kernel matrices of the kernel mentioned above. Then the kernel matrix
    associated to $c_1K_1(x,x') + c_2K_2(x,x')$ is $K=c_1K_1+c_2K_2$ which is
    postive-semidefinite because for any $v\in\mathbb{R}^n, v^\top(c_1K_1+c_2K_2)v
    = c_1(v^\top K_1v)+c_2(v^\top K_2v) \geq 0$, as both addends are greater or
    equal to 0 because of the positive-semidefinitedness of $K_1\text{ and }K_2$,
    and it is symmetric since the sum is commutative.
\end{proof}

\subsection{Kernel machines}
\label{subsec:kmachines}

What kernel machines try to accomplish is basically sample separation in a given
feature space.
This is achieved exploiting the information contained in the kernel matrix about
sample pairwise similarity, by defining the separation problem as an optimization
problem, constrained on those very similarity values, we are guaranteed to find
a global optimum by the properties of the kernel function, as detailed in
section~\ref{subsec:kernelfunc}.
Taking as an example setting non-linearly separable data in a binary classification
context, we will now go through the whole technique.
Being non linearly separable means that the data cannot be separated by a single
hyperplane in the input space, that is, the space in which the samples are defined.
A solution to this problem is to define a function $\phi$ that will map each
sample from the input space $X$ with $n$ dimensions, to another space $\mathcal{H}$
of much higher dimensionality, let's say $M >> n$, i.e. $\phi:X\to \mathcal{H}$.
Given a sample $x \in X$, the $j^{th}$ feature of $\phi(x)$ will be then be
computed by another non-linear function $\phi_j$, $\forall j \in \{1,\dots,M\}$.
This technique is employed because of the result brought on by Cover's Theorem
that states:

\begin{theorem}[Cover's Theorem]
    A complex pattern-classification problem, cast in a high-dimensional space
    non-linearly, is more likely to be linearly separable than in a low-dimensional
    space, provided that the space is not densely populated.
\end{theorem}

this result rely on the fact that projecting the data in an higher-dimensional
space than the input one, they will become more sparse hence more easily separable
with a linear separator.
Given these premises, and a value of $M$ high enough for the mapped samples to
be linearly separable a kernel machine looks for the optimal linear separator in
$\mathcal{H}$ and then maps it back to the input space as a non-linear separator.
To find this optimal separator the kernel machine has to deal with samples of
very high (possibly infinite) dimensionality and this could quickly render the
search for the solution to the optimization problem infeasible or too onerous.
Given that, in the problem formulation, the function $\phi$ only appears inside
the computation of dot products, these values can be calculated by a kernel
function (section~\ref{subsec:kernelfunc}) thus referring to $\mathcal{H}$
only implicitly and avoiding having to deal with a high number of dimensions
altogether using what is commonly known as the \emph{kernel trick}.

\subsubsection{Support Vector Machines}
\label{subsubsec:svm}
Here we will briefly describe one of the most tried kernel machines in the literature. 
\emph{Support Vector Machines} or SVM [Vapnick] rest on the principle of structural risk
minimization (section \ref{subsubsec:srm}) providing a trade-off between hypotheses
space complexity and the performances in fitting the training data.
It is a binary classifier but can be used to solve multi-class classification
problems with some additional work to combine multiple instances of binary
classifiers.
It is also a linear classifier by default and can be easily extended to the 
non-linear domain by using it in conjunction with (non-linear) kernel functions.
Given a feature space, this method tries to find the optimal hyperplane that
correctly separates the samples in such space, and separates them in the
most general way that is, maximising the \emph{margin} between each class.
In order to minimize the bound on true error (Section \ref{subsubsec:srm})
this method uses hypotheses spaces of crescent VC-dimension, to minimize the
VC-confidence while keeping a low empirical error.
There are two main settings in which this algorithm can work: when the data
is linearly separable in its original feature space and when it is not.
In the first case, since the empirical risk is going to be null, finding the
optimal hyperplane means finding the one that again minimizes the VC-dimension
which in turn is the one that maximizes the margin between the classes i.e.
the minimum distance between the hyperplane and the nearest sample.
Solving this problem equals to solving the following quadratic optimization
problem:

\begin{gather}
    \begin{aligned}
        & min_{\vec{w},b}\frac{1}{2}||\vec{w}||^2 \\
        & \text{s.t. } \forall i \in \{1,\dots, n\} : y_i(\vec{w}\cdot\vec{x_i} + b) - 1 \geq 0 
        \label{eq:lsvm}
    \end{aligned}
\end{gather}

where $\vec{w} \cdot \vec{x} + b$ defines the hyperplane.
Since the margin we are looking to maximize is inversely proportional to the norm
of $\vec{w}$, we want to minimize the latter while the $\frac{1}{2}$ factor stems
from mathematical convenience.
Equation \ref{eq:lsvm} shows the primal form of the problem, whose formulation
has a convex cost function and constraints, hence it can be solved more easily 
switching to its dual form that, given these premises, will have the same
optimal solution.
Thanks to the Kuhn-Tucker theorem [ref] we can formulate the dual form using
Lagrange's multipliers:

\begin{gather}
    \underset{\vec{w},b}{\mathrm{min}}\,\underset{\vec{\alpha}\geq 0}{\mathrm{max}}\,L(\vec{w},b,\vec{\alpha})
    \label{eq:ldual}
\end{gather}

where $L$ is the lagrangian function and $\vec{\alpha}$ the multipliers.

\begin{equation}
    L(\vec{w},b,\vec{\alpha}) = \frac{1}{2}||\vec{w}||^2 - \sum_{i=1}^n \alpha_i(y_i(\vec{w}\cdot\vec{x} + b) - 1)
    \label{eq:lagrange}
\end{equation}

The optimal solution to this problem is the saddle point obtained while minimizing
w.r.t. $\vec{w}\text{ and }b$ and maximizing w.r.t. $\vec{\alpha}$.

When reaching this point the gradient of the function $L$ must be null:

\begin{gather}
    \begin{aligned}
        & \frac{\delta L(\vec{w},b,\vec{\alpha})}{\delta\vec{w}} = 0 \iff \vec{w^*} = \sum^{n}_{i=1} y_i\alpha^*_i\vec{x_i}\\
        & \frac{\delta L(\vec{w},b,\vec{\alpha})}{\delta b} = 0 \iff \sum_{i=1}^{n} y_i\alpha^*_i = 0\\
        & \alpha^*_i[y_i(\vec{w^*}\cdot \vec{x_i}+b^*) - 1] = 0,\,i=1,\dots,n
        \label{eq:lagrangedual}
    \end{aligned}
\end{gather}

and this lets us get rid of the primal variables in the lagrangian function since
they can be expressed using the labelled samples and the multipliers thus hence
reducing the problem to the maximization of $L(\vec{\alpha})$.
Those samples $X_i$ for which the $\alpha^*_i$ are greater than zero are called
support vectors and are the ones closer to the hyperplane as can be seen in
figure ??.

The other case, that is when the data is not linearly separable, is solved in a
similar way but with the introduction of \emph{slack variables}, one for each
constraint:

\begin{equation}
    y_i(\vec{w}\cdot\vec{x} + b) \geq 1 - \xi_i
\end{equation}

These variables $\xi_i$, will account for the constraint violation by modifying the cost
function:

\begin{equation}
    \frac{1}{2}||\vec{w}||^2 + C \sum_{i=1}^n \xi_i
\end{equation}

where $C$ is a regularization parameter that will balance the trade-off between
the hypotheses space complexity and the number of non-separable samples.
Recalling the dual form of the previous case after the elimination of the primal
variables from the lagrangian function (Equations \ref{eq:lagrange} and \ref{eq:lagrangedual}),
we can now formulate our problem in the following way:

\begin{gather}
    \begin{aligned}
    & \underset{\vec{\alpha}}{\mathrm{max}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^n y_iy_j\alpha_i\alpha_j\vec{x_i}\vec{x_j}\\
    & \text{s.t. } \forall i \in \{1,\dots, n\} : 0 \leq \alpha_i \leq C,\, \sum_{i=1}^n y_i\alpha_i = 0 
    \end{aligned}
\end{gather}

This formulation differs from the previous one in that the dual variables (the $\alpha_i$s)
have $C$ as an upper bound.
The role of the slack variable $\xi_i$ is exemplified in figure ??.

The above solution has the major downside that cannot guarantee good performances
in highly non-separable data because of the limitation in the ways an hyperplane
can separate any feature space, i.e. through dichotomies.
A remedy to this fact is to use the technique exposed in Section \ref{subsec:kmachines}
while maintaining the same formulation explained above.
The effects of this technique on the classification task is visible in figure ??.

%----------------------------------------------------------------------------------------

\section{Kernels for structured data}
\label{sec:graphkernels}

In the previous section we introduced the theoretical foundations behind kernel
functions and the related learning methods.
The kernel learning approach is certainly convenient, the user needs to concentrate
its effort on the selection or design of the right kernel function for the problem
at hand, then couple it with one of the available general-purpose approaches to
get a full-fledged learning method.
That being said, it is clear that in this scenario the kernel function is the
component responsible for the main source of bias.
Nowadays a number of different kernel functions are available with varying degree
of complexity in order to tackle a variety of problems, most of which has data
represented in vectorial form.
The rise of problems where data is naturally better represented with a
structured form (Section \ref{subsubsec:examples}) has rendered necessary the
study of new methods to tackle them.
One way of dealing with structured data could be to actually get rid of the structure
deriving an \emph{ad-hoc} vectorial representation, in what is called a
\emph{structure-based} approach.
This method is expensive and cumbersome since it force the user to have a strong
knowledge of the task domain and to re-develop a new vectorial representation
for each task that she may encounter.

A more effective approach would be to adapt the methods to work directly on
structured data, and in this regard kernel methods are a good fit since the
decoupling between the kernel function and the kernel machine leaves the
only contact point between the data and the method in the function itself.
These methods maps the sample instances into high dimensional vectors and do it
in a general way without any knowledge of the underlying data beside its structure.
Furthermore this mapping is done implicitly meaning that they also have an
advantage from a computational standpoint.

Designing such kernel functions that can deal with structured data is a process
that has to balance between designing functions that are expressive enough to
produce some learning and the computational efficiency that it is required in
order to effectively perform the learning process.
As an example of this trade-off consider a kernel on graphs that counts all the
matching subgraphs between two input graphs.
This is certainly one of the most expressive kernel possible if not the most expressive
at all.
Even if such enumeration would exist, the kernel would still need to solve the
graph isomorphism problem several times, but that is clearly infeasible since graph
isomorphism is NP-hard.
Algorithmic complexity for a kernel function is a major point since the kernel has
to be computed for each pairs of samples, rendering even a quadratic complexity
a hindering factor for its application.

An theoretically sound way to design efficient and effective kernel functions for
structured data has been proposed by Haussler \cite{haussler} and will be explained
in the next section.
The main concept behind his work is that we need to restrict the number and type
of features considered for each structure.
Again, the choice of the substructures that would become features deeply affects
the learning outcome and is one of the major sources of bias in the whole learning
process, possibly leading to greatly differing performances according to different
datasets.
For what concern the scope of this thesis we will see in the following sections
some examples of kernels defined on trees and on graphs, all of which are instances
of the framework proposed by Haussler.

\subsection{The Framework: Convolution Kernels}

\subsection{Kernels For Trees}
\subsubsection{Sub Tree Kernel}
\subsubsection{Augmented Sub Tree Kernel}

\subsection{Kernels For Graphs}
\subsubsection{ODD kernels}
\label{subsubsec:odd}
% explain ordered graph decomposition kernels
\subsubsection{Fast Subtree kernels}
% explain FS kernels

\subsection{Latest Improvements on Graph Kernel Methods}
\label{subsec:kernel}

\subsubsection{Kernels with contextual information}
\label{subsec:context}
% explain contexts

\section{Kernels Combination Techniques}
\label{sec:tech}
%kernels SUM: descrption, pros, cons

\subsection{Multiple Kernel Learning}
\label{subsec:mkl}
% what is it and why it matters
% relieve the user from careful kernel design => lots of weak kernels

\subsubsection{EasyMKL}
\label{subsubsec:easymkl}
% principles
% design

\subsection{Multiple Kernel Learning On Graphs}
\label{subsec:gmkl}
% the technique

%----------------------------------------------------------------------------------------

% vim: spell spelllang=en_gb
