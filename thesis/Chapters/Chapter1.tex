% Chapter 1

\chapter{Introduction} % Main chapter title
\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

This chapter presents a panoramic view of the field of study on which this thesis
bases itself.
First we introduce the fundamental concepts of machine learning, followed by
some more topic-specific discussion, then an overview of the main contribution
is given, followed by the document outline.

\section{Relationship Between Data And Knowledge}

The current amount of digital data produced daily is estimated to be in the realm of 
ZettaBytes ($1~ZB = 10^{12}~GB$) (citation needed).
Of this amount, only a small fraction sees actual processing and interpretation,
mainly because while the flow of information is steady, the available resources
devoted to the task are often limited (ibidem).
Moreover in some cases the rapidity at which a meaningful interpretation of the 
raw data has to be produced is critical.
Hence, just the plain fact of owning the data does not implies that some useful 
meaning can be derived from it; in fact a whole discipline was born with the
intent of derive \emph{knowledge} that ``we don't know we don't know''.

This is indeed one of the many achievement of \emph{Data Mining}, a prominent field for
professionals and scientists alike.
Most of its techniques draw from a variety of other fields like Statistics,
Computer Science, Information Theory and Math just to mention a few.
This discipline rests its theoretical foundation upon the field of \emph{Machine Learning},
the discipline that studies algorithms and techniques that allows a machine
to learn a concept (or an approximation of it) from a set of examples.
These algorithms start from limited evidence and try to build up a general model
by refining an initial hypothesis.
Machine Learning is mostly useful when either an exact solution to a very
difficult problem is computationally infeasible or the problem specification
is rather vague or not unambiguously defined, leaving to the algorithm the burden
of exploration.

One of the pivotal concepts of this field is data \emph{representation}.
For the most part, data is collected and stored in vectorial form (think of
a database rows for instance) and a lot of machine learning techniques are
tailored to this ubiquitous representation.
A vector is just a tuple of values with some kind of identifier for each of them
and represent little to no other information per se hence data that comes in
this format is considered unstructured data.

In the last decades some approaches tried to address tasks whose data could
not be represented in vectorial form or whose representation this way would cause
a great deal of information loss.
More specifically some work has been done to devise methods that could learn
directly from graphs, a very useful kind of \emph{structured data} that sees many
important real-world applications nowadays, especially in biology and chemistry.

%----------------------------------------------------------------------------------------

\section{Why Structured Data}
As previously mentioned, data in vectorial form retain practically no inherent
information about the record it represent, it's just a collection of values with
no other relation between themselves but to be in the same tuple.
Looking at a vector for instance, it is not immediately clear if the values share
some dependency, if they are correlated and in which measure, and what is the
individual relevancy to the prediction task at hand, if any.
To overcome these question a number of analysis exist, which aim to add some
implicit knowledge to the data but they remain heuristic at best.

\subsection{Enter The Graphs}
On the other hand we have more structured records, and since this thesis will
focus on data encoded as graphs

\subsection{Examples}

%----------------------------------------------------------------------------------------

\section{Learning on Graphs}

%----------------------------------------------------------------------------------------

\section{Thesis Contribution}
% mkl should be one of the approaches to combine in addition of being the combining
%tool.

Nowadays we can rely on a fast, state-of-art $MKL$ implementation \cite{easymkl}
so we decided to exploit the inherent combining power of this approach for our
purposes.
By combining a number of weak kernels, derived from selected graph kernel
learning approaches, into a single learning process we wanted to determine if an
effective and overall performance improvement in target prediction was possible.

Moreover the whole process of parameters selection has been embedded inside the
learning phase to further improve the whole process (i.e. no need to pre-select
some catch-all optimal parameters combination).

%----------------------------------------------------------------------------------------

\section{Outline}

%----------------------------------------------------------------------------------------

% vim: spell spelllang=en_gb
