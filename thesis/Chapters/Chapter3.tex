\chapter{Graph Kernels Combination}
\label{Chapter3}

This chapter details the contribution of the present work.
We start by analysing the motives behind kernels combination and in particular
behind graph kernels combination (section \ref{sec:why}).
Then, in the following sections, we proceed with examining the existing techniques
(section \ref{sec:tech}), we discuss in detail the proposed methodology (section
\ref{sec:meth}) and finally give an overview of the main optimization choices (sections
\ref{sec:opt}, and \ref{sec:inc})

\section{Why Combining Kernels}
\label{sec:why}

In the previous chapter we described a number of approaches (section \ref{sec:}

\subsection{The Case For Graph Kernels}

\section{Existing combination techniques}
\label{sec:tech}
%kernels SUM: descrption, pros, cons

\section{The Proposed Methodology}
\label{sec:meth}
%thorough description
 
\subsection{Embedded Parameters Selection}
\label{subsec:parameters}
%fix

As previously mentioned, since we were using a $MKL$ approach, the whole set of
input kernel matrices derived from the grid of parameters has been fed into the
algorithm at each step.

This choice stem from the fact that the implementation we used for $MKL$ gives us
back a set of weights, one for each of the input kernel matrices; these weights,
multiplied by their related matrix which then get summed together give us the
best linear combination of the input kernels.
While a weight is not to be taken as an assessment of the goodness of a single
kernel matrix (since they are optimal as a combination), a very low weight can
indeed be a sure sign that the related matrix has given a very poor contribution
to the classification task.

%\subsubsection{An alternative approach: hierarchical MKL}
%\label{subsec:hierarchy}
% describe the approach in detail (pseudocode)

%    \begin{algorithm}[h]
%        \caption{MKL nested K-fold cross validation training}\label{puremkl}
%        \begin{algorithmic}[1]
%            \Procedure{ModelSelection}{$kernels*$}\Comment{input is a list of kernel matrices}
%                \ForAll{a}{A}
%                \While{$r\not=0$}\Comment{We have the answer if r is 0}
%                    \State $a\gets b$
%                    \State $b\gets r$
%                    \State $r\gets a\bmod b$
%                \EndWhile\label{euclidendwhile}
%                \State \textbf{return} $b$\Comment{The gcd is b}
%        \EndProcedure
%    \end{algorithmic}
%    \end{algorithm}
%In Algorithm~\ref{puremkl}, variable \var{foobar} (in line~\ref{foobar}), corresponds to\ldots

\subsection{Kernels and Feature Spaces}
\label{subsec:features}
% analyse the combinations? Analyse the bucketized version?

%----------------------------------------------------------------------------------------

\section{Method optimization}
\label{sec:opt}
% memory consumption (+ alternate methods constant RAM)

%----------------------------------------------------------------------------------------

\section{Incremental kernels calculation} 
\label{sec:inc}
The kernels we analysed in this study present feature space representations
that are in a subset relation; this prompted us to devise a method to calculate
such representations in an incremental fashion trying to gain a significant
speed-up.

\begin{algorithm}
    \caption{The devised algorithm to incrementally compute the explicit
    features space representation for the available ODD kernels, namely
    $ODD_{ST}$, $ODD_{STC}$, $ODD_{STP}$, $ODD_{STPC}$.}
    \label{alg:incremental}
    \begin{algorithmic}[1]
        \ForAll{$kernel \in Kernels$}
            \State $\phi_{kernel} \gets [0,\dots,0]$
        \EndFor

        \ForAll{$v \in V_g$}
            \State $f \gets \{\}$
            \State $size \gets \{\}$
            \State $dag \gets DAG_h(v, g)$
            \ForAll{$u \in \Call{ReverseTopologicalOrder}{dag}$}
                \ForAll{$d \in \{0,\dots,diam(dag)-|sp(v,u)|\}$}
                    \If{$d=0$}
                        \State $f_{u,0} \gets \kappa(L(u))$
                        \State $size_{u,0} \gets 1$
                        \State add $f_{u,0}$ to $\phi_{ODD_{ST}}$
                        \State add $f_{u,0}$ to $\phi_{ODD_{STP}}$
                    \Else
                        \State $(S_1,\dots,S_{\rho(u)}) \gets \Call{Sort}{
                        f_{ch_1(u),d-1},f_{ch_2(u),d-1},\dots,f_{ch_{\rho(u)}(u),d-1}}$
                        \State $f_{u,d} \gets \kappa(L(u)\lceil{}S_1\#S_2\#\dots\#S_{\rho(u)}\rfloor)$
                        \State $size_{u,d} \gets 1 + \sum_{i=1}^{\rho(u)}size_{ch_i(u),d-1}$
                        \ForAll{$ch \in children(u)$}
                            \State assign $f_{ch,d-1}$ as a context to $f_{u,d}$
                            \State compute weight of $f_{ch,d-1}$
                            \State add contextualized feature to $\phi_{ODD_{STC}}$
                            \State add contextualized feature to $\phi_{ODD_{STPC}}$
                        \EndFor
                        \State add $f_{u,d}$ to $\phi_{ODD_{ST}}$
                        \State add $f_{u,d}$ to $\phi_{ODD_{STP}}$
                    \EndIf
                    \If{$u=v$}
                        \State add $f_{u,d}\circ{}c$ to $\phi_{ODD_{STC}}$
                        \State add $f_{u,d}\circ{}c$ to $\phi_{ODD_{STPC}}$
                    \EndIf

                    \State Compute $ODD_{STP}$ and $ODD_{STPC}$ peculiar features
                    and contexts and add them to the relevant $\phi$ in a similar
                    fashion\label{line:stp}
                \EndFor
            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm}

In Algorithm~\ref{alg:incremental}, line~\ref{line:stp} refers to the sub-procedures
defined in \cite{nnavarin, rtesselli}.
As one can see, given a graph instance, the algorithm is able to build and
collect the features in one pass thus maintaining a performance of $O(n)$ where
$n$ is the dimension of the input (i.e. the number of graphs) versus the
previous approach that would require $O(m \cdot n)$ with $m$ being the number of
kernels being computed.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.4]{Figures/kernel_times}
    \caption{Times in seconds required to compute the kernels $ODD_{ST}$ and 
    $ODD_{STC}$ incrementally and sequentially on a selection of datasets.}
    \label{fig:times}
\end{figure}

The plot in figure~\ref{fig:times} shows the measured times on a variety of
datasets (see section~\ref{subsec:datasets} for further details on each one)

%----------------------------------------------------------------------------------------

