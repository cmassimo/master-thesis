\chapter{Graph Kernels Combination}
\label{Chapter3}

This chapter details the contribution of the present work.
We start by analysing the motives behind kernels combination and in particular
behind graph kernels combination (section \ref{sec:why}).
Then, in the following sections, we proceed with examining the existing techniques
(section \ref{sec:tech}), we discuss in detail the proposed methodology (section
\ref{sec:meth}) and finally give an overview of the main optimization choices (sections
\ref{sec:opt}, and \ref{sec:inc})

\section{Why Combining Kernels}
\label{sec:why}

In the previous chapter we described a number of approaches (section \ref{sec:graphkernels})
and the relative improvements that has been presented in the recent years.

\subsection{The Case For Graph Kernels}

\section{Existing combination techniques}
\label{sec:tech}
%kernels SUM: descrption, pros, cons

\section{The Proposed Methodology}
\label{sec:meth}
%thorough description
 

This choice stem from the fact that the implementation we used for $MKL$ gives us
back a set of weights, one for each of the input kernel matrices; these weights,
multiplied by their related matrix which then get summed together give us the
best linear combination of the input kernels.
While a weight is not to be taken as an assessment of the goodness of a single
kernel matrix (since they are optimal as a combination), a very low weight can
indeed be a sure sign that the related matrix has given a very poor contribution
to the classification task.

% easyMKL altrimenti ciao

\subsection{Immersed Model Selection}
\label{subsec:parameters}
% butta tutte le matrici in un colpo solo
One of the main advantages of the proposed method is that it permits to embed
the otherwise cumbersome process of hyper-parameter optimization (section \ref{subsubsec:grid})
inside the learning phase in a completely transparent way, which will be hereby
described.

It is custom practice to perform hyper-parameter optimization employing a grid
search technique.
In our setting this would translate to having to compute one kernel matrix
for all the possible combinations of values deriving from every set of values
chosen for each parameter both from the kernel and the selected learning
algorithm, for instance the $h$ and $\lambda$ parameters for the $ODD$ kernels
and the $C$ parameter if using the SVM.
Then every thusly computed set of kernels would have been fed to the $MKL$
algorithm to be evaluated via cross-validation.
Let $l,m,n$ be the cardinality respectively of the three sets of values above,
the cross-validation instances would have been $l\cdot m\cdot n$.

A first improvement over the above scenario is to adopt a multiple kernel learning
approach and feed the algorithm the kernels computed from the grid all at once.
One of the advantages of this approach is that instead of selecting one combination
of parameters thus restricting the generalization power of the resulting model,
the whole set of parameters combination concurs in the model determination hence
increasing the chances of it being more general.

Given the fact that our $MKL$ implementation of choice has an algorithmic
complexity that is not dependent on the cardinality of the input but rather on
its dimensions, one drawback of this approach is that the memory consumption
invariably increases with the number of kernel thus making this approach still
computationally prone to the curse of dimensionality.

Section \ref{sec:opt} deals with a possible solution to this problem.

\subsection{Kernels and Feature Spaces}
\label{subsec:features}


% buckets => leva un parametro

%----------------------------------------------------------------------------------------

\section{Method optimization}
\label{sec:opt}
% memory consumption (+ alternate methods constant RAM)

Memory consumption is basically bound to $O(m\cdot n^2)$ with $m$ being the number of kernels and
$n$ the number of samples while being in practice $O(2\cdot (m\cdot n^2))$.
%----------------------------------------------------------------------------------------

\section{Incremental kernels calculation} 
\label{sec:inc}
The kernels we analysed in this study present feature space representations
that are in a subset relation; this prompted us to devise a method to calculate
such representations in an incremental fashion trying to gain a significant
speed-up.

\begin{algorithm}
    \caption{The devised algorithm to incrementally compute the explicit
    features space representation for the available ODD kernels, namely
    $ODD_{ST}$, $ODD_{STC}$, $ODD_{STP}$, $ODD_{STPC}$.}
    \label{alg:incremental}
    \begin{algorithmic}[1]
        \ForAll{$kernel \in Kernels$}
            \State $\phi_{kernel} \gets [0,\dots,0]$
        \EndFor

        \ForAll{$v \in V_g$}
            \State $f \gets \{\}$
            \State $size \gets \{\}$
            \State $dag \gets DAG_h(v, g)$
            \ForAll{$u \in \Call{ReverseTopologicalOrder}{dag}$}
                \ForAll{$d \in \{0,\dots,diam(dag)-|sp(v,u)|\}$}
                    \If{$d=0$}
                        \State $f_{u,0} \gets \kappa(L(u))$
                        \State $size_{u,0} \gets 1$
                        \State add $f_{u,0}$ to $\phi_{ODD_{ST}}$
                        \State add $f_{u,0}$ to $\phi_{ODD_{STP}}$
                    \Else
                        \State $(S_1,\dots,S_{\rho(u)}) \gets \Call{Sort}{
                        f_{ch_1(u),d-1},f_{ch_2(u),d-1},\dots,f_{ch_{\rho(u)}(u),d-1}}$
                        \State $f_{u,d} \gets \kappa(L(u)\lceil{}S_1\#S_2\#\dots\#S_{\rho(u)}\rfloor)$
                        \State $size_{u,d} \gets 1 + \sum_{i=1}^{\rho(u)}size_{ch_i(u),d-1}$
                        \ForAll{$ch \in children(u)$}
                            \State assign $f_{ch,d-1}$ as a context to $f_{u,d}$
                            \State compute weight of $f_{ch,d-1}$
                            \State add contextualized feature to $\phi_{ODD_{STC}}$
                            \State add contextualized feature to $\phi_{ODD_{STPC}}$
                        \EndFor
                        \State add $f_{u,d}$ to $\phi_{ODD_{ST}}$
                        \State add $f_{u,d}$ to $\phi_{ODD_{STP}}$
                    \EndIf
                    \If{$u=v$}
                        \State add $f_{u,d}\circ{}c$ to $\phi_{ODD_{STC}}$
                        \State add $f_{u,d}\circ{}c$ to $\phi_{ODD_{STPC}}$
                    \EndIf

                    \State Compute $ODD_{STP}$ and $ODD_{STPC}$ peculiar features
                    and contexts and add them to the relevant $\phi$ in a similar
                    fashion\label{line:stp}
                \EndFor
            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm}

In Algorithm~\ref{alg:incremental}, line~\ref{line:stp} refers to the sub-procedures
defined in \cite{nnavarin, rtesselli}.
As one can see, given a graph instance, the algorithm is able to build and
collect the features in one pass thus maintaining a performance of $O(n)$ where
$n$ is the dimension of the input (i.e. the number of graphs) versus the
previous approach that would require $O(m \cdot n)$ with $m$ being the number of
kernels being computed.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.4]{Figures/kernel_times}
    \caption{Times in seconds required to compute the kernels $ODD_{ST}$ and 
    $ODD_{STC}$ incrementally and sequentially on a selection of datasets.}
    \label{fig:times}
\end{figure}

The plot in figure~\ref{fig:times} shows the measured times on a variety of
datasets (see section~\ref{subsec:datasets} for further details on each one)

%----------------------------------------------------------------------------------------

