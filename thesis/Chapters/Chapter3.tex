\chapter{An alternative to hyper-parameter selection}
\label{Chapter3}

This chapter details the contribution of the present thesis.
We discuss the current state of the art in Section \ref{sec:hyper}, then we detail
the proposed methodology (Section \ref{sec:method}) and finally an overview
of the main optimization choices (Sections \ref{subsec:opt}, and \ref{subsec:inc}) is
given.

\section{Hyper-parameter selection}
\label{sec:hyper}
% what is it and why is it done
% methods need parameter tuning.
Machine learning methods perform a non-exhaustive search on the hypothesis space
(Section \ref{subsec:sup}), therefore they have a set of parameters to influence
and direct their search strategy, in order to find optimal solutions in reasonable time.
These parameters define a parameter space that is the set of all possible combinations of
values for all the different parameters.
% non covex space
The size of a particular parameter space is given by the product of the cardinality
of the domain associated with each parameter hence, an exhaustive search would
prove to be too onerous if not infeasible.

% this can be accomplished in several ways: searches, optimization, etc, thus
% all are computationally demanding since you have to visit a lot of the parameter
% space which grows exponentially.

Often this problem can be avoided considering a subset of the space (see Section
\ref{subsubsec:grid}), thus reducing the computational complexity of an exhaustive
search, or performing a limited random search on the full space.
All these approaches aim to minimize the generalization error of a given method,
so they need to perform an adequate scan of the parameter space, which usually
remains computationally heavy.
Another way of accomplish this task is to devise algorithm-specific parameter
optimization techniques as is the case for \cite{chappelle}.

Kernel methods in particular have two types of hyper-parameters.
Kernel functions have parameters that shape the feature space associated with them
to balance the trade-off between expressiveness and computational efficiency.
Kernel machines have parameters that usually play the role of regularization
factors i.e. to keep overfitting under control.
In this case it is clear how much the parameter selection process for the kernel function
can affect the overall learning process, more so if the kernel function has to be
selected as well.

% kernel methods have two types of params
% kernel params (needed to shape the feature space and balance the trade-off between expressiveness and efficiency)
% methods params (needed to regularize the machine behaviour)
% kernel params greaty affect the overall performance in case one need to validate

% different kernels
% Eg:
To give a quick example, consider a kernel derived from the $ODD$ framework, say
the $ODDK_{ST}$ (Section \ref{subsubsec:odd}), let $l,m$ be the cardinalities of the value sets on which its parameters
$h$ and $\lambda$ need to be validated; Let the SVM be the kernel machine of choice and
$n$ the cardinality of the value set for its parameter $C$.
Employing a grid search technique, i.e. an exhaustive search over the subset
of the parameter space thus defined, parameter selection would require $l\cdot m\cdot n$
searches.

A first improvement over this scenario is to adopt a kernels combination approach
and use all the kernel computed from the parameter grid at once.
With the above described settings the number of search instances
shrinks from $l\cdot m\cdot n$ to $n$, hence potentially reducing the size of the
whole process by orders of magnitude.
A visual comparison of these two approaches is given in Figure \ref{fig:comparison}.

\begin{figure}[h]
    \centering
    \begin{subfigure}{.391\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/comp1}
        \caption{Single kernel approach}
        \label{fig:comp1}
    \end{subfigure}\qquad
    \begin{subfigure}{.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/comp2}
        \caption{Kernel combination approach}
        \label{fig:comp2}
    \end{subfigure}
    \caption{Comparison between the standard approach (a) and the combination approach (b).
    $K_1,\dots,K_R$ refers to the kernels computed from the $R$ combinations
    derived from the parameters grid. In (a) each kernel is used to perform a full learning
    cycle, $R$ in total. In (b) all the $R$ kernels are used together
    as the input of the learning algorithm.
    In the settings described above $R=l\cdot m$.}
    \label{fig:comparison}
\end{figure}

%----------------------------------------------------------------------------------------

\section{Proposed methodology}
\label{sec:method}
% given the scheme proposed in the previous section:
% intro MKL
% combination of the grid
% orthogonalization
The idea we propose here takes a cue from the scheme in Figure \ref{fig:comp2}.
As discussed in Section \ref{subsec:mkl}, MKL methods generally provide a consistent
way of using different, possibly weak, kernels to build a model which is the result
of their composition.
While these methods are usually employed to boost the performance of the single
kernels, we approach the combination technique from a different perspective.
The main requisite of our idea is the possibility to combine a potentially
large number of kernels without a significant loss of computational performances.
The EasyMKL implementation (Section \ref{subsec:easymkl}) was deemed a good
choice because of its state-of-the-art performances and linear complexity
bound on the input size both in time and space.

We therefore propose a novel way of performing the \emph{whole} learning phase.
This consists in avoiding the kernel hyper-parameter selection process entirely by considering
all the kernels according to a finite subset of the kernel parameter space and composing
them together to perform the learning task, employing EasyMKL capabilities as the kernel
machine.
Algorithm \ref{alg:method} presents the pseudo-code for the methodology.

\begin{algorithm}
    \caption{
        High level implementation of the proposed methodology.
        The \textsc{computeKernel} function returns a kernel according to the
        selected kernel function $k$ and one hyper-parameters tuple from the
        parameter grid $kernelparams$ on the given $data$.
        \textsc{validationRoutine} represent a generic validation framework which
        takes $EasyMKL$ as the chosen kernel machine, the vector $hyperparams$
        as its associated hyper-parameters to be validated and the kernels
        in $K$ as the new data (see Section \ref{sec:kernel}).
    }
    \label{alg:method}
    \begin{algorithmic}[1]
        \State $K \gets \{\}$
        \Comment{$K$ is list of the input kernels}
        \label{line:kernels}
        \ForAll{$f \in KernelFunctions$}
            \ForAll{$i \in \{1,\dots,n\}$}
            \Comment{$n$ is the size of the parameter grid}
            \State $K_{(f,i)} \gets \Call{computeKernel}{f,~kernelparams_i,~data}$
            \EndFor
        \EndFor

        \State $performances \gets \Call{validationRoutine}{EasyMKL,~hyperparams,~K}$

%        \ForAll{$i \in \{1,\dots,\mathit{numfolds}\}$}
%        \State $\mathit{trainingset}, \mathit{testset} \gets \Call{split}{M, i}$
%            \label{line:osubsets}
%            \ForAll{$\Lambda \in params$}
%                \ForAll{$j \in \{1,\dots,\mathit{numfolds}\}$}
%                \State $\mathit{innertrainingset}, \mathit{validationset} \gets \Call{split}{\mathit{trainingset}, j}$
%                    \label{line:isubsets}
%                    \State $model \gets \Call{easyMKL}{\Lambda,\mathit{innertrainingset}}$
%                    \State validate $model$ with $\mathit{validationset}$
%                \EndFor
%            \EndFor
%            
%            \State select the best $(model,\Lambda)$ resulting from the inner loop
%            \State $\mathit{finalmodel} \gets \Call{easyMKL}{\Lambda,\mathit{trainingset}}$
%            \State test $\mathit{finalmodel}$ with $\mathit{testset}$
%        \EndFor

        \State \Return $performances$
        \Comment an unbiased performance estimation.
    \end{algorithmic}
\end{algorithm}

%\begin{figure}[ht]
%    \centering
%    \includegraphics[scale=0.5]{Figures/nested}
%    \caption{The method illustrated. First the dataset, composed of a list of
%    kernel matrices get split into a training set and a test set (1).
%    Then a $k$-fold cross-validation is performed on the training set ($k$ was set to
%    10 in our case) (2) and the best resulting model is selected according to some
%    performance measure (3). The selected model gets re-trained on the whole initial
%    training set (4) and finally gets tested against the test set that was left out
%    during the entire process (5). At the end of the outer loop of the nested
%    cross-validation, the best model is again selected according to the chosen
%    metric (6).}
%    \label{fig:method}
%\end{figure}

% kernel machine select the model 
% bias reduction
% overfitting reductiooooonnnnnn
By combining all the kernels in one learning phase, we let the kernel machine
take advantage of the whole information brought on by the single kernels.
This in turn results in bias reduction since the algorithm does not rely on a single
measure of similarity but rather collects a contribution from each kernel thus
in the end employing a more expressive measure defined on the composition of all
the underlying feature spaces.

The reduction in the number of hyper-parameters to validate has also the effect
of reducing the overall chance of overfitting given the reduction of degrees of
freedom in the hypothesis approximation.

%Even if the memory consumption of the adopted MKL algorithm is linear w.r.t.
%the number of considered matrices, in our scenario where hundreds of matrices
%have to be combined it is still possible that the matrices will not physically
%fit into memory.
%
%Section \ref{sec:opt} deals with a possible algorithmic solution to this problem,
%while in Section \ref{subsec:features} we will show how it is possible to combine the idea here
%described with another technique used in multiple kernel learning to further
%improve the feature spaces and reduce the computational burden and space
%requirements eliminating one hyper-parameter.

%Even if in principle MKL algorithms can deal with large number of kernels, the
%memory consumption problem previously described will manifest itself even for
%datasets of modest dimensions, because it mainly depends on the parameters grid size,
%i.e. on the number of kernels to combine.
%
%Beside adopting a sampling strategy on the dataset sacrificing some learning
%potential, a solution consists in reducing the number of hyper-parameters
%thus reducing the dimensionality of the parameter space obtaining a consequent
%reduction in the number of kernels to be computed

% ok to combine all kernel but it will work better if the information provided
% is orthogonal
% recently proposed work devise a general method of orthogonalizing graph kernels
% even if in principle this orthogonalization is a source of bias the fact that 
% we can use more than one kernel at once mitigate it since our methodology can
% can deal with multiple kernel functions (i.e. multiple feature subdivision
% strategies)

\subsection{Feature space improvement}
\label{subsec:features}

MKL algorithms perform better when the information provided by each kernel is
orthogonal with respect to each others (see Section \ref{subsec:easymkl}).
Combining kernels with the above described methodology do not satisfy this property
in general, since they are computed by the same function thus representing very similar feature
spaces.
Recently some works has been done to further orthogonalize the information
provided by graph kernels \cite{gmkl}.
The technique consists in partitioning the feature space of a given kernel in such
a way that the resulting subsets define features that are independent from each others.
This is equivalent to decompose the original feature space in a set of non-overlapping
sub-spaces.
The approach has therefore been integrated in the kernel computation phase of
the proposed methodology.

It is clear how in principle the way in which this decomposition is carried on can be a
strong source of bias since once again the similarity measure between the samples
is being affected.
Resting the proposed methodology on a MKL approach partly remedy this problem since
kernels computed by different functions, i.e. according to different partitioning
strategies, can be combined at once so that each one can contribute to the final
outcome.

% join these two parts
A brief description of the orthogonalization techniques adopted for the graph
kernels considered in this study follows.

The $ODDK$ feature space is composed of trees from tree-visits over the DAGs
deriving from the DAG decomposition on the original graph (Section \ref{subsubsec:odd}).
This space has an inherent hierarchical structure that is, if a tree of a given
height is present in the explicit representation of a graph, also all of its proper
sub-trees are \cite{gmkl}.
Hence, grouping features of different height to different sub-spaces makes it so
two dependent features never end up contributing to the same kernel.
Moreover in this way the standard weighing scheme of the $ODD$ kernel can be
delegated to the EasyMKL algorithm, because the weight assigned to a particular
kernel by the algorithm would directly correlate with the underlying feature
sub-space thus rendering the $\lambda$ parameter of the $ODD$ kernel superfluous.

The fast subtree (WL) kernel has an already orthogonally structured feature
space since each feature extracted at iteration $i \in \{0,\dots,h\}$ (see Section \ref{subsubsec:fs})
is independent from all the feature extracted at iteration $j \in \{1,\dots,h\}\setminus \{i\}$,
hence the partition technique consists in considering each iteration as a different
feature space.

The most direct consequence of the adoption of this techniques is that it is
possible to remove some hyper-parameter thus typically reducing the
overall number of kernels.

%----------------------------------------------------------------------------------------

\section{Some considerations on complexity}

Implementing our methodology as it has been defined in Algorithm \ref{alg:method},
results in a memory consumption bound of $O(c \cdot (R\cdot m^2))$ with $c$ being
the multiplicative constant that accounts for the number of copies of the kernels
a specific implementation may need to instantiate, $R$ the number of kernels and
$m$ the number of samples in the dataset, although we empirically determined that
it can be kept below $2\cdot (R\cdot m^2)$.
This complexity measure can be extrapolated from  Algorithm \ref{alg:method}:
at any given point during its execution, the kernels vector is in
memory with a size equal to $R\cdot m^2$ (line \ref{line:kernels});
the validation routine of choice will then have to make a copy of the kernels vector
for each step of the parameter selection phase concerning the kernel machine.
Our final implementation memory consumption reflects this bound, however this
memory consumption rate can still be daunting for some applications,
for this reason we designed a different approach that employs a slight modification
of the standard EasyMKL algorithm \cite{aiolli2015easymkl} in order to keep a
constant level of memory occupation.

\subsection{A constant-space implementation}
\label{subsec:opt}
This approach relies on the fact that the EasyMKL algorithm implementation can
be divided in two main phases, namely the margin optimization problem (KOMD)
solution and the weights calculation phase (Section \ref{subsec:easymkl}).
Moreover during these phases the algorithm mostly works on a unique kernel which is
the normalized sum of the input kernels.
Hence we divided the original algorithm in two different functions and did
the kernel calculation, normalization and summation in a single pass before each phase,
therefore maintaining only one kernel in memory at one time thus achieving constant
space occupation.

This method has obviously one major drawback which is the latency derived from
having to either compute each kernel multiple times during the execution and
finally having to normalize and sum it.
This is inevitable because we are shifting the inherent burden of the data dimensionality
from space to time which can still be useful for some applications
depending on the available resources or imposed constraints.
This latter problem is however mitigated by the fact that the dramatic decrease
of memory consumption allows in practice for much better parallelization.

An high level overview of this approach is given as Algorithm \ref{alg:method_me}.
\begin{algorithm}
    \caption{
        High level implementation of the constant space implementation.
        The \textsc{easyMKLopt} and \textsc{easyMKLweights} functions implement
        the two phases of the EasyMKL algorithm as discussed in Section \ref{subsec:opt}.
        The \textsc{computeSumKernel} function is defined in the next part
        (Algorithm \ref{alg:compute_sum}).
    }

    \label{alg:method_me}
    \begin{algorithmic}[1]
%        \ForAll{$i \in \{1,\dots,\mathit{numfolds}\}$}
%            \ForAll{$\Lambda \in params$}
%                \ForAll{$j \in \{1,\dots,\mathit{numfolds}\}$}
                    %\State $tset \gets \Call{computeSumKernel}{Ks,grid,data,innerfold}[0]$
                    \State $K \gets \Call{computeSumKernel}{KernelFunctions,kernelparams,data}$
                    \State $\Call{easyMKLopt}{\Lambda,~K}$
                    \Comment first phase of EasyMKL
                    \State $model \gets \Call{easyMKLweights}{\Lambda,~K}$
                    \Comment second phase of EasyMKL
                    \State \Return model
%                    \State validate $model$ with $vset$
%                \EndFor
%            \EndFor
%            
%            \State select the best $model,\Lambda$ resulting from the inner loop
%            %\State $training\_set \gets \Call{computeSumKernel}{Ks,grid,data,outer\_fold}[0]$
%            \State $trainingset,testset \gets \Call{computeSumKernel}{Ks,grid,data,i}$
%            \State $\Call{easyMKLopt}{\Lambda,trainingset}$
%            \State $finalmodel \gets \Call{easyMKLweights}{\Lambda,trainingset}$
%            \State test $finalmodel$ with $testset$
%        \EndFor

%        \State \Return an unbiased performance estimation.
        \algstore{mme}
    \end{algorithmic}
\end{algorithm}

For each learning phase there are two calls to the \textsc{computeSumKernel}
function, one of which is hidden inside the \textsc{easyMKLweights} function.
Algorithm \ref{alg:compute_sum} defines the auxiliary function \textsc{computeSumKernel},
in charge of managing the kernels computation and sum in Algorithm \ref{alg:method_me}.

\begin{algorithm}
    \caption{
        Here an auxiliary function for computing, normalizing and summing the
        kernels to be combined is shown.
    }
    \label{alg:compute_sum}
    \begin{algorithmic}[1]
        \algrestore{mme}
        \Function{computeSumKernel}{$KernelFunctions,~kernelparams,~dataset$}
            \State initialize $K$ the null kernel
            \Comment $m$ is the size of $dataset$
            \ForAll{$f \in KernelFunctions$}
            \ForAll{$i \in \{1,\dots,n\}$}
            \Comment $n$ is the size of the parameter grid
            \State $k \gets \Call{computeKernel}{f,~kernelparams_i,~dataset}$
                    \State $\Call{traceNormalize}{k}$
                    \State $K \gets K+k$
                \EndFor
            \EndFor
            \State \Return $K$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Incremental kernels calculation} 
\label{subsec:inc}
The kernels we analysed in this study present feature space representations
that are often in a subset relation; this prompted us to devise a method to
calculate such representations in an incremental fashion trying to gain a
significant speed-up.

\begin{algorithm}
    \caption{The devised algorithm to incrementally compute the explicit
    features space representation for the available $ODD$ kernels, namely
    $ODDK_{ST}$, $TCK_{ST}$, $ODDK_{ST+}$, $TCK_{ST+}$.
    The $ReverseTopologicalOrder$ function returns a list of nodes in reverse
    order with respect to the topological order.
    The $sort$ function sorts the trees representing the explicit features
    according to their size.
    The notation for this algorithm has been derived from \cite{rtesselli}.
    }
    \label{alg:incremental}
    \begin{algorithmic}[1]
        \ForAll{$f \in KernelFunctions$}
            \State $\phi_{kernel} \gets [0,\dots,0]$
        \EndFor

        \ForAll{$v \in V_g$}
            \State $f \gets \{\}$
            \State $size \gets \{\}$
            \State $dag \gets DAG_h(v, g)$
            \ForAll{$u \in \Call{ReverseTopologicalOrder}{dag}$}
                \ForAll{$d \in \{0,\dots,diam(dag)-|sp(v,u)|\}$}
                    \If{$d=0$}
                        \State $f_{u,0} \gets \kappa(L(u))$
                        \State $size_{u,0} \gets 1$
                        \State add $f_{u,0}$ to $\phi_{ODDK_{ST}}$
                        \State add $f_{u,0}$ to $\phi_{ODDK_{ST+}}$
                    \Else
                        \State $(S_1,\dots,S_{\rho(u)}) \gets \Call{Sort}{
                        f_{ch_1(u),d-1},f_{ch_2(u),d-1},\dots,f_{ch_{\rho(u)}(u),d-1}}$
                        \State $f_{u,d} \gets \kappa(L(u)\lceil{}S_1\#S_2\#\dots\#S_{\rho(u)}\rfloor)$
                        \State $size_{u,d} \gets 1 + \sum_{i=1}^{\rho(u)}size_{ch_i(u),d-1}$
                        \ForAll{$ch \in children(u)$}
                            \State assign $f_{ch,d-1}$ as a context to $f_{u,d}$
                            \State compute weight of $f_{ch,d-1}$
                            \State add contextualized feature to $\phi_{TCK_{ST}}$
                            \State add contextualized feature to $\phi_{TCK_{ST+}}$
                        \EndFor
                        \State add $f_{u,d}$ to $\phi_{ODDK_{ST}}$
                        \State add $f_{u,d}$ to $\phi_{ODDK_{ST+}}$
                    \EndIf
                    \If{$u=v$}
                        \State add $f_{u,d}\circ{}c$ to $\phi_{TCK_{ST}}$
                        \State add $f_{u,d}\circ{}c$ to $\phi_{TCK_{ST+}}$
                    \EndIf

                    \State Compute $ODDK_{ST+}$ and $TCK_{ST+}$ peculiar features
                    and contexts and add them to the relevant $\phi$ in a similar
                    fashion\label{line:stp}
                \EndFor
            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm}

In Algorithm~\ref{alg:incremental}, line~\ref{line:stp} refers to the sub-procedures
defined in \cite{nnavarin, rtesselli}.
As one can see, given a graph instance, the algorithm is able to build and
collect the features in one pass thus maintaining a performance of $O(n)$ where
$n$ is the dimension of the input (i.e. the number of graphs) versus the
previous approach that would require $O(m \cdot n)$ with $m$ being the number of
kernels being computed.
A similar version of this algorithm has been implemented for the computation
of the kernels discussed in Section \ref{subsec:features}.
Again, this procedures has largely been derived from the work done in \cite{nnavarin, rtesselli}.
An analysis if the computation times will be provided in Section \ref{subsec:time_results}.

%----------------------------------------------------------------------------------------

% vim: spell spelllang=en_gb
